---
title: "models"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readtext)
library(stringr)
library(rpart)
library(adabag)
library(randomForest)
library(gbm)
library(xgboost)
library(ipred)
library(pROC)
```
These models use the z alizadehsani set

Separate testing and training data
```{r}
load("z.Rdata")

set.seed(1111) #random seed  

# 70% of the data is training, 30% is testing
train.row = sample(1:nrow(z), 0.7*nrow(z))  
data.train = z[train.row,]  
data.test = z[-train.row,]

```

# Naive Bayes
Implement a Naive bayes model using the e1071 package.
Inputs: data.train data set, data.test data set
outputs: naiveb the naive bayes model
         nb_predictions the predicted outcomes of the data.test set
         a confusion matrix of the results
```{r}
library(e1071)

naiveb<- naiveBayes(Cath~., data=data.train)

nb_Predictions=predict(naiveb,data.test)
#Confusion matrix to check accuracy
table(nb_Predictions,data.test$Cath)

# Calculate area under the ROC curve
roc_obj <- roc(as.numeric(data.test$Cath), as.numeric(nb_Predictions))
auc(roc_obj)
```
59% accuracy
Area under the curve: 0.6839

# decision tree
Implement a decision tree using the rpart package.
Inputs: data.train data set, data.test data set
outputs: tree the tree model
         nb_predictions the predicted outcomes of the data.test set
         a confusion matrix of the results
```{r}

tree<- rpart(Cath~., data.train, method="class")

tree

plot(tree)
text(tree)

set.seed(1234)

tree_prediction<- predict(tree, data.test, type = "class")
table(actual	=	data.test$Cath,	predicted	=	tree_prediction)

# Area under ROC curve
roc_obj <- roc(as.numeric(data.test$Cath), as.numeric(tree_prediction))
auc(roc_obj)
```
75.82% accuracy
Area under the curve: 0.7387




## Random Forest 

```{r}
set.seed(1234)
forest<- randomForest(as.factor(Cath)~., data = data.train, ntree=1000)

forest

table(actual	=	data.test$Cath,	predicted	=	predict(forest, data.test))


# Area under ROC curve
roc_obj <- roc(as.numeric(data.test$Cath), as.numeric(predict(forest, data.test)))
auc(roc_obj)
```

79.12% accurate 
Area under the curve: 0.7169


##XG boost

```{r}
xg.train<-data.train
xg.test<- data.test


# to numeric
xg.train$Obesity <-as.numeric(xg.train$Obesity)
xg.train$CRF <-as.numeric(xg.train$CRF)
xg.train$CVA <-as.numeric(xg.train$CVA)
xg.train$Airway.disease <-as.numeric(xg.train$Airway.disease)
xg.train$Thyroid.Disease <-as.numeric(xg.train$Thyroid.Disease)
xg.train$CHF <-as.numeric(xg.train$CHF)
xg.train$DLP <-as.numeric(xg.train$DLP)
xg.train$Weak.Peripheral.Pulse <-as.numeric(xg.train$Weak.Peripheral.Pulse)
xg.train$Lung.rales <-as.numeric(xg.train$Lung.rales)
xg.train$Systolic.Murmur <-as.numeric(xg.train$Systolic.Murmur)
xg.train$Diastolic.Murmur <-as.numeric(xg.train$Diastolic.Murmur)
xg.train$Dyspnea <-as.numeric(xg.train$Dyspnea)
xg.train$Atypical <-as.numeric(xg.train$Atypical)
xg.train$Nonanginal <-as.numeric(xg.train$Nonanginal)
xg.train$Exertional.CP <-as.numeric(xg.train$Exertional.CP)
xg.train$LowTH.Ang <-as.numeric(xg.train$LowTH.Ang)
xg.train$LVH <-as.numeric(xg.train$LVH)
xg.train$Poor.R.Progression <-as.numeric(xg.train$Poor.R.Progression)
xg.train$BBB <-as.numeric(xg.train$BBB)
xg.train$VHD <-as.numeric(xg.train$VHD)
xg.train$Sex <-as.numeric(xg.train$Sex)
xg.train$Current.Smoker <-as.numeric(xg.train$Current.Smoker)
xg.train$Cath <-as.numeric(xg.train$Cath)

# to numeric
xg.test$Obesity <-as.numeric(xg.test$Obesity)
xg.test$CRF <-as.numeric(xg.test$CRF)
xg.test$CVA <-as.numeric(xg.test$CVA)
xg.test$Airway.disease <-as.numeric(xg.test$Airway.disease)
xg.test$Thyroid.Disease <-as.numeric(xg.test$Thyroid.Disease)
xg.test$CHF <-as.numeric(xg.test$CHF)
xg.test$DLP <-as.numeric(xg.test$DLP)
xg.test$Weak.Peripheral.Pulse <-as.numeric(xg.test$Weak.Peripheral.Pulse)
xg.test$Lung.rales <-as.numeric(xg.test$Lung.rales)
xg.test$Systolic.Murmur <-as.numeric(xg.test$Systolic.Murmur)
xg.test$Diastolic.Murmur <-as.numeric(xg.test$Diastolic.Murmur)
xg.test$Dyspnea <-as.numeric(xg.test$Dyspnea)
xg.test$Atypical <-as.numeric(xg.test$Atypical)
xg.test$Nonanginal <-as.numeric(xg.test$Nonanginal)
xg.test$Exertional.CP <-as.numeric(xg.test$Exertional.CP)
xg.test$LowTH.Ang <-as.numeric(xg.test$LowTH.Ang)
xg.test$LVH <-as.numeric(xg.test$LVH)
xg.test$Poor.R.Progression <-as.numeric(xg.test$Poor.R.Progression)
xg.test$BBB <-as.numeric(xg.test$BBB)
xg.test$VHD <-as.numeric(xg.test$VHD)
xg.test$Sex <-as.numeric(xg.test$Sex)
xg.test$Current.Smoker <-as.numeric(xg.test$Current.Smoker)
xg.test$Cath <-as.numeric(xg.test$Cath)
# make into a matrix
xg.train<- data.matrix(xg.train)
xg.test<-data.matrix(xg.test)
Cath<- as.numeric(data.train$Cath)


```

This is the first xgboost using linear regression, then classifying each prediction as Cath or not Cath with split on 0.5
```{r}
xg<- xgboost(data=xg.train[,-56], label = Cath, max.depth=6, nrounds = 25)


importance <- xgb.importance(feature_names = colnames(xg.train), model = xg)
importance

pred <- predict(xg, xg.test[,-56])
prediction <- as.numeric(pred > 1.5)
err <- mean(as.numeric(pred > 1.5) != xg.test[,56]-1)
print(paste("test-error=", err))

# for Cath column 1= Cad, 2=Normal

table(actual	=	xg.test[,56]-1,	predicted	=	prediction)
```
73.6% accuracy. This Xgboost probably isn't the best model because it doesn't estimate binary variables.

This Xgboost uses binary/logistic regression:
```{r}
Cath=Cath-1
xg2<- xgboost(data=xg.train[,-56], label = Cath, max.depth=4, nrounds = 25, objective = "binary:logistic")


importance <- xgb.importance(feature_names = colnames(xg.train), model = xg2)
importance

pred <- predict(xg2, xg.test[,-56])
prediction <- as.numeric(pred > 0.50)

table(actual	=	xg.test[,56]-1,	predicted	=	prediction)




roc_obj <- roc(xg.test[,56]-1, prediction)
auc(roc_obj)
```
76.9% accuracy for max.depth=6, nround=25
75.8% accuracy for max.depth=7, nround=25
75.8% accuracy for max.depth=7, nround=50
79.1% accuracy for max.depth=4/3, nround=25

Area under ROC curve= 0.7481

# SVM 
SVM model
```{r}
library(e1071)

# SVM takes a matrix
# also remove  CHR ,Exertional.CP , LowTH.Ang
svm.train<-data.train[ -c(16, 30, 31) ]
svm.test<-data.test[ -c(16, 30, 31) ]
svm.train<- data.matrix(svm.train)
svm.test<-data.matrix(svm.test)

# this is the model
svmmodel<-svm(Cath~., data=svm.train)

# predict the test set
pred<-predict(svmmodel, svm.test)
prediction <- as.numeric(pred > 1.5)
table(actual	=	xg.test[,56]-1,	predicted	=	prediction)

roc_obj <- roc(xg.test[,56]-1, prediction)
auc(roc_obj)
```
80.2% accurate

area under ROC curve 0.7487

One article had a decision tree as the best model, so i'll repeat that one
https://www.datacamp.com/community/tutorials/decision-trees-R 
```{r}
library("ISLR")
library("tree")

tree.2 = tree(Cath~., data=data.train)
cv.tree = cv.tree(tree.2, FUN = prune.misclass)

plot(cv.tree)
```
```{r}
# pick tree size=11
prune.tree2 = prune.misclass(tree.2, best = 11)
plot(prune.tree2)
text(prune.tree2, pretty=0)

treep<- predict(prune.tree2, data.test, type = "class")
table(actual	=	data.test$Cath,	predicted	=	treep)

roc_obj <- roc(as.numeric(data.test$Cath),as.numeric(treep))
auc(roc_obj)
```
70.3% accuracy
AUC ROC 0.6737

# Bagging
```{r}
set.seed(12345)

bagmodel<-bagging(Cath~., data=data.train, m = 5, ntree = 1000, mtry = NULL, trace = T)


bagpred<- predict(bagmodel, data.test)
table(actual	=	data.test$Cath,	predicted	=	bagpred)

roc_obj <- roc(as.numeric(data.test$Cath),as.numeric(bagpred))
auc(roc_obj)

save(bagmodel, file="baggingmodel.rda")
```
81.31% Accuracy
Area under the curve: 0.796




# variable importance
```{r}
# tree
tree$variable.importance


# boosting
boost$importance

# random forest
forest$importance


```

## Testing Results
The models are:
- naiveb: Naive bayes using e1071
- tree: simple rpart decision tree
- forest: random forest 
- xg2: Xgboost 79.1% accuracy for max.depth=4/3, nround=25
- svmmodel: SVM 80.2% accuracy 
- bagmodel: Bagging using multiple classification trees


Model       Accuracy        AUC ROC
naiveb      59%             0.6839
tree        75.82%          0.7387
forest      79.12%          0.7169
xg2         79.1%           0.7481
svmmodel    80.2%           0.7487
bagmodel    81.31%          0.796


# Making Predictions
This is a prototype of the function to create predictions in the final web application. It takes inputs from the user and uses the best model (bagging) to predict Cad or Normal.
Inputs: 
Outputs: Cad or Normal
```{r}
# load the model
load(file = "baggingmodel.rda")

# Ask for inputs

print("Please enter the following details")
age1 <- readline(prompt="Age: ")
weight1<-readline(prompt="Weight: ")
length1<-readline(prompt="Height in cm: ")
bmi1<-readline(prompt="BMI: ")

# means from FBS

userdata<-data.frame(Age=as.numeric(age1), Weight=as.numeric(weight1), Length=as.numeric(length1), Sex= "Male", BMI=as.numeric(bmi1), DM=0, HTN=0, Current.Smoker= "0", Ex.Smoker="1", FH=0, Obesity= "N", CRF= "N", CVA="N", Airway.disease= "N", Thyroid.Disease= "N", CHF= "N", DLP= "N", BP=as.numeric(140), PR=as.numeric(70), Edema= 0, Weak.Peripheral.Pulse= "N", Lung.rales= "N", Systolic.Murmur= "N", Diastolic.Murmur="N", Typical.Chest.Pain= 0, Dyspnea= "Y", Function.Class=0, Atypical= "Y", Nonanginal= "N", Exertional.CP="N", LowTH.Ang= "N", Q.Wave=0, St.Elevation=0, St.Depression= 0, Tinversion= 0, LVH= "N", Poor.R.Progression= "N", BBB="LBBB", FBS= 119, CR=1, TG=150, LDL=105, HDL=40.2,  BUN=17.5, ESR= 19.5, HB=13.1, K=4.2, Na= 141,WBC= 7562,  Lymph=31, Neut=60, PLT= 221, EF.TTE= 47, Region.RWMA=0, VHD= "mild")

# make sure column names are the same
names(userdata) <- names(z[2,-56])

# bind the row of user data to one row from the original data set so that it inherits the factor levels
userdata<-rbind(z[2,-56], userdata)

# predict the outcome
prediction<-predict(bagmodel, userdata[2,])

# Print Response
ifelse(prediction=="Normal", "Your test result is normal. You most likely do not have coronary artery disease, but please see a doctor if you have any concerns.", "Your test result is Coronary artery disease. Please note that this test is not 100% accurate, and you can only be diagnosed by a medical professional.")
```

Exertional.CP , LowTH.Ang and CHR??? dont have enough variation, set both to N